{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "annual-bridges",
   "metadata": {},
   "source": [
    "# DSCI 525 - Web and Cloud Computing\n",
    "## Project: Daily Rainfall Over NSW, Australia\n",
    "## Milestone 1: Tackling Big Data on Your Laptop \n",
    "#### Authors: Group 24 Huanhuan Li, Nash Makhija and Nicholas Wu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-angola",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cognitive-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd\n",
    "from memory_profiler import memory_usage\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "amazing-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import rpy2.rinterface\n",
    "import rpy2_arrow.pyarrow_rarrow as pyra\n",
    "import pyarrow.feather as feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "collaborative-lease",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "increasing-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for this notebook was adapted from DSCI 525 course notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-disposition",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will work with large dataset in Pandas and vanilla CSV files. Typically these are not the best for dealing with large data.. The purpose of this exercise is for us to get exposure to working with some useful tools for working with big data, such as DASK, Apache Arrow package, Feather and Parquet files formats.   \n",
    "\n",
    "The dataset we will be using can be found [here](https://figshare.com/articles/dataset/Daily_rainfall_over_NSW_Australia/14096681). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-powder",
   "metadata": {},
   "source": [
    "## 1) Downloading the data\n",
    "\n",
    "We will start with downloading the data from [figshare](https://figshare.com/articles/dataset/Daily_rainfall_over_NSW_Australia/14096681) to our local computer using the [figshare API](https://docs.figshare.com/) with the help of `requests` library.   \n",
    "\n",
    "The code below defines the endpoint and header info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incorporate-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary metadata\n",
    "article_id = 14096681  # this is the unique identifier of the article on figshare\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"../data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-daisy",
   "metadata": {},
   "source": [
    "We start with sending a GET request to list the available files from the endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "southwest-wrapping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_link_only': False,\n",
       "  'name': 'daily_rainfall_2014.png',\n",
       "  'supplied_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'computed_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'id': 26579150,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579150',\n",
       "  'size': 58863},\n",
       " {'is_link_only': False,\n",
       "  'name': 'environment.yml',\n",
       "  'supplied_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'computed_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'id': 26579171,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579171',\n",
       "  'size': 192},\n",
       " {'is_link_only': False,\n",
       "  'name': 'README.md',\n",
       "  'supplied_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'computed_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'id': 26586554,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26586554',\n",
       "  'size': 5422},\n",
       " {'is_link_only': False,\n",
       "  'name': 'data.zip',\n",
       "  'supplied_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'computed_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'id': 26766812,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766812',\n",
       "  'size': 814041183},\n",
       " {'is_link_only': False,\n",
       "  'name': 'get_data.py',\n",
       "  'supplied_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'computed_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'id': 26766815,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766815',\n",
       "  'size': 4113}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  # this contains all the articles data, feel free to check it out\n",
    "files = data[\"files\"]             # this is just the data about the files, which is what we want\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-intelligence",
   "metadata": {},
   "source": [
    ">   \n",
    "According to the url from metadata, we are going to download the file named \"data.zip\" to the data folder.   \n",
    ">  \n",
    "Once the data.zip is successfully downloaded, we are going to extract the zip file programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pressing-vegetation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.49 s, sys: 6.44 s, total: 12.9 s\n",
      "Wall time: 14min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "files_to_dl = [\"data.zip\"]\n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "invisible-remedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.4 s, sys: 2.83 s, total: 20.3 s\n",
      "Wall time: 20.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-territory",
   "metadata": {},
   "source": [
    ">  \n",
    "We can confirm by looking at the data foler that \"data.zip\" has been downloaded and extracted successfully. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-peoples",
   "metadata": {},
   "source": [
    "## 2) Combine data CSVs\n",
    "\n",
    "From the zip file, we extracted 28 .csv files. 27 of them are machine learning models. We are now going to merge the .csv files into one and add an extra column called \"model\" that identifies the name of the model. \n",
    "\n",
    "Note, we extracted the model name from the file names. For example, for file name \"SAM0-UNICON_daily_rainfall_NSW.csv\", the model name is SAM0-UNICON. \n",
    "\n",
    "The tool we chose to use for this task is `Pandas`. \n",
    "\n",
    "Let's start by inspecting what an individual .csv file looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tested-migration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>3.293256e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>1.047658e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932835</th>\n",
       "      <td>2014-12-27 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>2.951144e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932836</th>\n",
       "      <td>2014-12-28 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>2.257118e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932837</th>\n",
       "      <td>2014-12-29 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>1.204670e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932838</th>\n",
       "      <td>2014-12-30 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>2.632404e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932839</th>\n",
       "      <td>2014-12-31 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>3.431610e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1932840 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        time  lat_min  lat_max  lon_min  lon_max  \\\n",
       "0        1889-01-01 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "1        1889-01-02 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "2        1889-01-03 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "3        1889-01-04 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "4        1889-01-05 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "...                      ...      ...      ...      ...      ...   \n",
       "1932835  2014-12-27 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "1932836  2014-12-28 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "1932837  2014-12-29 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "1932838  2014-12-30 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "1932839  2014-12-31 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "\n",
       "         rain (mm/day)  \n",
       "0         3.293256e-13  \n",
       "1         0.000000e+00  \n",
       "2         0.000000e+00  \n",
       "3         0.000000e+00  \n",
       "4         1.047658e-02  \n",
       "...                ...  \n",
       "1932835   2.951144e-02  \n",
       "1932836   2.257118e-01  \n",
       "1932837   1.204670e-01  \n",
       "1932838   2.632404e-02  \n",
       "1932839   3.431610e-02  \n",
       "\n",
       "[1932840 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### just listing to get an idea how individual file looks like \n",
    "use_cols = ['time', 'lat_min', 'lat_max', 'lon_min', 'lon_max', 'rain (mm/day)']\n",
    "df = pd.read_csv(\"../data/ACCESS-CM2_daily_rainfall_NSW.csv\", usecols=use_cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-wallpaper",
   "metadata": {},
   "source": [
    "> We can see that one .csv file has more than 1.9 million rows and 6 columns. So to combine all 27 files, presumably, we would have more than 51 million rows. To compare run times and memory usages of `Pandas` on different machines, we will use magic command `%%time` from IPYTHON and `%%memit` from memory_profiler to record these info. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-pilot",
   "metadata": {},
   "source": [
    "The following code extract and add model name from the .csv files, and combine all .csv files into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "liable-conservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 477.45 MiB, increment: 0.07 MiB\n",
      "CPU times: user 4min 31s, sys: 9.92 s, total: 4min 41s\n",
      "Wall time: 4min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "# Shows time that regular python takes to merge file\n",
    "# Join all data together\n",
    "## here we are using a normal python way of merging the data \n",
    "files = glob.glob('../data/*NSW.csv')\n",
    "df = pd.concat((pd.read_csv(file, index_col=0, usecols=use_cols)\n",
    "                .assign(model=file[8:file.index(\"_daily\")])\n",
    "                for file in files)\n",
    "              )\n",
    "df.to_csv(\"../data/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "smooth-natural",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6G\t../data/combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh ../data/combined_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fluid-march",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.4 s, sys: 11.5 s, total: 56.9 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_pandas = pd.read_csv(\"../data/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "inside-smart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62467843, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dental-hardware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.244226e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.217326e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.498125e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.251282e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.270161e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time    lat_min    lat_max   lon_min   lon_max  \\\n",
       "0  1889-01-01 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "1  1889-01-02 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "2  1889-01-03 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "3  1889-01-04 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "4  1889-01-05 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "\n",
       "   rain (mm/day)            model  \n",
       "0   4.244226e-13  MPI-ESM-1-2-HAM  \n",
       "1   4.217326e-13  MPI-ESM-1-2-HAM  \n",
       "2   4.498125e-13  MPI-ESM-1-2-HAM  \n",
       "3   4.251282e-13  MPI-ESM-1-2-HAM  \n",
       "4   4.270161e-13  MPI-ESM-1-2-HAM  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-bulletin",
   "metadata": {},
   "source": [
    "#### Summary of Observation on Run Times and Memory Usage Comparison on Different Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-dover",
   "metadata": {},
   "source": [
    "|Name|Machine| Total Time Taken to Concatenate and Create .csv File | Peak Memory Usage | Time taken to Load |\n",
    "|---|---| --- | --- | --- |\n",
    "|Huanhuan|Windows| 5min 48s | 427 MiB | 1min 7s |\n",
    "|Nash|macOS| 6min 7s | 359 MiB | 1min 15s |\n",
    "|Nick|macOS| 4min 44s | 397 MiB | 50.5s |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-carpet",
   "metadata": {},
   "source": [
    "> Summary: The run times and memory usages on our team members' machine are all similar in concatenating and creating the .csv files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-madison",
   "metadata": {},
   "source": [
    ">Nash's laptop initially had storage issues due to hard drive being close to full storage. Nash had to free up space before he was successfully able to create combined_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-affect",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-grant",
   "metadata": {},
   "source": [
    "## 3) Load the Combined CSV to Memory and Perform a Simple EDA\n",
    "\n",
    "\n",
    "There are a number of ways to load the combined CSV file to memory. Pandas and R by default load the entire data frame to memory at once. \n",
    "\n",
    "Space issue arises quickly when the data we are trying to process is bigger than our RAM. In our case, the combined_data.csv is 5.6 GB. \n",
    "\n",
    "In this section, we are going to explore some approaches to reduce memory usage while performing an EDA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-longitude",
   "metadata": {},
   "source": [
    "### Approach 1. Load the Entire Dataframe to Memory Using Pandas (Baseline for Comparison)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "square-pension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "NorESM2-MM          3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "TaiESM1             3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "SAM0-UNICON         3541153\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "MRI-ESM2-0          3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM5-0           1609650\n",
      "INM-CM4-8           1609650\n",
      "KIOST-ESM           1287720\n",
      "FGOALS-g3           1287720\n",
      "AWI-ESM-1-1-LR       966420\n",
      "NESM3                966420\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "MPI-ESM1-2-LR        966420\n",
      "NorESM2-LM           919800\n",
      "CanESM5              551880\n",
      "BCC-ESM1             551880\n",
      "Name: model, dtype: int64\n",
      "peak memory: 9448.29 MiB, increment: 3986.23 MiB\n",
      "CPU times: user 49.3 s, sys: 9.49 s, total: 58.8 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "df_pandas = pd.read_csv(\"../data/combined_data.csv\")\n",
    "print(df_pandas[\"model\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-defensive",
   "metadata": {},
   "source": [
    ">  \n",
    "Our baseline for comparison is to use Pandas to load the entire data to memory. \n",
    ">  \n",
    "The above code loads the combined_data.csv to memory and performs a simple EDA to calculate counts of values in the \"model\" column.   \n",
    ">  \n",
    "We can see that the peak memory is 9,448.29 MiB and the CPU and wall time is close to one minute. \n",
    ">  \n",
    "Let's explore some other approaches to see if we can reduce the memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-turkish",
   "metadata": {},
   "source": [
    "### Approach 2. Changing `dtype` of the Data \n",
    "\n",
    "\n",
    "One approach to reduce memory usage is to change the `dtype` of the original data. \n",
    "  \n",
    "We can see from the output below, that five of the six columns are of float64 datatype. We will convert them to float32 and check if the memory usage is reduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dried-monaco",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time              object\n",
       "lat_min          float64\n",
       "lat_max          float64\n",
       "lon_min          float64\n",
       "lon_max          float64\n",
       "rain (mm/day)    float64\n",
       "model             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "industrial-pierce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage with float64: 2498.71 MB\n",
      "Memory usage with float32: 1249.36 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory usage with float64: {df_pandas[['lat_min','lat_max','lon_min', 'lon_max', 'rain (mm/day)']].memory_usage().sum() / 1e6:.2f} MB\")\n",
    "print(f\"Memory usage with float32: {df_pandas[['lat_min','lat_max','lon_min', 'lon_max', 'rain (mm/day)']].astype('float32', errors='ignore').memory_usage().sum() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "hungarian-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting df_pandas into float32 for columns with float type values\n",
    "df_pandas_float32 = df_pandas.copy()\n",
    "df_pandas_float32[\"lat_min\"] = df_pandas[\"lat_min\"].astype('float32')\n",
    "df_pandas_float32[\"lat_max\"] = df_pandas[\"lat_max\"].astype('float32')\n",
    "df_pandas_float32[\"lon_min\"] = df_pandas[\"lat_min\"].astype('float32')\n",
    "df_pandas_float32[\"lon_max\"] = df_pandas[\"lat_max\"].astype('float32')\n",
    "df_pandas_float32[\"rain (mm/day)\"] = df_pandas[\"rain (mm/day)\"].astype('float32')\n",
    "\n",
    "#saving the dataframe of float32 to file\n",
    "df_pandas_float32.to_csv(\"../data/combined_data_float32.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "genuine-mongolia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "TaiESM1             3541230\n",
      "CMCC-ESM2           3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "NorESM2-MM          3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "SAM0-UNICON         3541153\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "FGOALS-f3-L         3219300\n",
      "MRI-ESM2-0          3037320\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "KIOST-ESM           1287720\n",
      "FGOALS-g3           1287720\n",
      "MPI-ESM1-2-LR        966420\n",
      "NESM3                966420\n",
      "AWI-ESM-1-1-LR       966420\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "NorESM2-LM           919800\n",
      "BCC-ESM1             551880\n",
      "CanESM5              551880\n",
      "Name: model, dtype: int64\n",
      "peak memory: 9193.91 MiB, increment: 1797.70 MiB\n",
      "CPU times: user 55.8 s, sys: 25.2 s, total: 1min 20s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "#loading the float32 dataframe to memory and perform a simple EDA for value counts of model column\n",
    "df_pandas_float32 = pd.read_csv(\"../data/combined_data_float32.csv\")\n",
    "print(df_pandas_float32[\"model\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-crawford",
   "metadata": {},
   "source": [
    "> Changing the `dtype` in the dataframe make the performance slightly better. The peak memory usage has decreased to 9,193 MiB and both CPU and wall time increased slightly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-politics",
   "metadata": {},
   "source": [
    "### Approach 3. Loading in Chunks  \n",
    "\n",
    "Another approach is to load the dataframe in chunks. \n",
    "\n",
    "The following code helps us to explore loading the dataframe in two different chunk size, 10 million per chunk and 1 million per chunk. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-announcement",
   "metadata": {},
   "source": [
    "#### Chunksize = 10 million:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "remarkable-momentum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "AWI-ESM-1-1-LR       966420\n",
      "BCC-CSM2-MR         3035340\n",
      "BCC-ESM1             551880\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "CanESM5              551880\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "FGOALS-f3-L         3219300\n",
      "FGOALS-g3           1287720\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "KIOST-ESM           1287720\n",
      "MIROC6              2070900\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "MPI-ESM1-2-HR       5154240\n",
      "MPI-ESM1-2-LR        966420\n",
      "MRI-ESM2-0          3037320\n",
      "NESM3                966420\n",
      "NorESM2-LM           919800\n",
      "NorESM2-MM          3541230\n",
      "SAM0-UNICON         3541153\n",
      "TaiESM1             3541230\n",
      "dtype: int64\n",
      "peak memory: 6838.55 MiB, increment: 1604.71 MiB\n",
      "CPU times: user 48.3 s, sys: 5.43 s, total: 53.7 s\n",
      "Wall time: 54.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "counts = pd.Series(dtype=int)\n",
    "for chunk in pd.read_csv(\"../data/combined_data.csv\", chunksize=10_000_000): #loading with 10 million per chunk\n",
    "    counts = counts.add(chunk[\"model\"].value_counts(), fill_value=0)\n",
    "print(counts.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-alert",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-integrity",
   "metadata": {},
   "source": [
    "#### chunksize = 1 million:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "elect-smith",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "AWI-ESM-1-1-LR       966420\n",
      "BCC-CSM2-MR         3035340\n",
      "BCC-ESM1             551880\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "CanESM5              551880\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "FGOALS-f3-L         3219300\n",
      "FGOALS-g3           1287720\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "KIOST-ESM           1287720\n",
      "MIROC6              2070900\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "MPI-ESM1-2-HR       5154240\n",
      "MPI-ESM1-2-LR        966420\n",
      "MRI-ESM2-0          3037320\n",
      "NESM3                966420\n",
      "NorESM2-LM           919800\n",
      "NorESM2-MM          3541230\n",
      "SAM0-UNICON         3541153\n",
      "TaiESM1             3541230\n",
      "dtype: int64\n",
      "peak memory: 5265.30 MiB, increment: 0.04 MiB\n",
      "CPU times: user 50.2 s, sys: 5.34 s, total: 55.5 s\n",
      "Wall time: 56.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "counts = pd.Series(dtype=int)\n",
    "for chunk in pd.read_csv(\"../data/combined_data.csv\", chunksize=1_000_000): #loading with 1 million per chunk\n",
    "    counts = counts.add(chunk[\"model\"].value_counts(), fill_value=0)\n",
    "print(counts.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-remains",
   "metadata": {},
   "source": [
    ">   \n",
    "We can see that loading in 10 million per chunk requires 7,605 MiB in peak memory usage, which is less than using Pandas to load all at once. \n",
    ">  \n",
    "Loading in 1 million per chunk requires only 4,235 MiB in peak memory usage. This is significantly more efficient than using Pandas. However, we also notice that the CPU and wall time remains roughly the same in all these approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-motivation",
   "metadata": {},
   "source": [
    "### Approach 4. Load using DASK  \n",
    "\n",
    "Lastly, we will explore using [DASK](https://dask.org). \n",
    "\n",
    "DASK is a scalable python library. It does the chunking and parallel execution for us, so we don't have to manually take care of it using the chunk_size for chunking up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "incoming-resort",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "TaiESM1             3541230\n",
      "NorESM2-MM          3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "SAM0-UNICON         3541153\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "MRI-ESM2-0          3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM5-0           1609650\n",
      "INM-CM4-8           1609650\n",
      "KIOST-ESM           1287720\n",
      "FGOALS-g3           1287720\n",
      "MPI-ESM1-2-LR        966420\n",
      "NESM3                966420\n",
      "AWI-ESM-1-1-LR       966420\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "NorESM2-LM           919800\n",
      "BCC-ESM1             551880\n",
      "CanESM5              551880\n",
      "Name: model, dtype: int64\n",
      "peak memory: 6522.68 MiB, increment: 1465.07 MiB\n",
      "CPU times: user 1min 11s, sys: 16 s, total: 1min 27s\n",
      "Wall time: 34.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# dask way\n",
    "\n",
    "df_dask = dd.read_csv(\"../data/combined_data.csv\")\n",
    "print(df_dask[\"model\"].value_counts().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-interest",
   "metadata": {},
   "source": [
    "### Discussion on Observations\n",
    "\n",
    "- Loading the entire data to memory using `Pandas` all at once takes the longest in wall time and the highest memory usage. \n",
    "- If we change the columns with float64 data type to float32, the memory usage of the file reduced significantly from 2,498 MB to 1,249 MB. By converting the data type, we reduced the peak memory usage from 9,448 MB to 9,193 MB. \n",
    "- Loading in chunks reduced peak memory usage but the CPU and sys time for running the cell was about the same as loading with Pandas. We tried with two different chunksize: 10 million and 1 million. The processing time is similar but the peak memory usage decreased significantly to 6,838 MB and 5,265 MB respectively. \n",
    "- Loading with Dask reduced peak memory usage to 6,522 MB. It also reduced wall time significantly by almost half. \n",
    "- In summary, out of the approaches investigated, loading in chunksize of 1 million requires the least amount of peak memory. However, if we are interested in reducing both memory usage and wall time, loading with DASK is the best way to go. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-excess",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-daily",
   "metadata": {},
   "source": [
    "## 4) Perform a Simple EDA in R  \n",
    "\n",
    "To perform an EDA in R, we will need to transfer the dataframe from Python to R first.   \n",
    "\n",
    "In this section, we will write our combined dataframe into some advanced file formats and compare their performance while doing a simple EDA in R. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-shanghai",
   "metadata": {},
   "source": [
    "### 1. Store the Data in Different Formats\n",
    "\n",
    "#### Arrow file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "removable-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "#Loading library\n",
    "library(\"arrow\");\n",
    "library(\"dplyr\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "mental-flexibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 6157.95 MiB, increment: 962.28 MiB\n",
      "CPU times: user 17.6 s, sys: 9.9 s, total: 27.5 s\n",
      "Wall time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "dataset = ds.dataset(\"../data/combined_data.csv\", format=\"csv\")\n",
    "## this is of arrow table format\n",
    "table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-prompt",
   "metadata": {},
   "source": [
    "#### Feather format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "toxic-breathing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.14 s, sys: 12.6 s, total: 17.7 s\n",
      "Wall time: 6.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# experiment in writing in feather format \n",
    "feather.write_feather(table, '../data/combined_data.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-chart",
   "metadata": {},
   "source": [
    "#### Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "regular-fence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.34 s, sys: 1.06 s, total: 9.4 s\n",
      "Wall time: 9.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## writing as a single parquet \n",
    "pq.write_table(table, '../data/combined_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "horizontal-relay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.3 s, sys: 12.4 s, total: 31.7 s\n",
      "Wall time: 29.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## writing as a partitioned parquet \n",
    "pq.write_to_dataset(table, '../data/combined_data_partitioned.parquet',partition_cols=['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "civil-progressive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6G\t../data/combined_data.csv\n",
      "1.0G\t../data/combined_data.feather\n",
      "544M\t../data/combined_data.parquet\n",
      "1.1G\t../data/combined_data_partitioned.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "# Check the size of different format\n",
    "du -sh ../data/combined_data.csv\n",
    "du -sh ../data/combined_data.feather\n",
    "du -sh ../data/combined_data.parquet\n",
    "du -sh ../data/combined_data_partitioned.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-enzyme",
   "metadata": {},
   "source": [
    ">  \n",
    "We can see that both Feather and Parquet have reduced the file size significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-bridge",
   "metadata": {},
   "source": [
    "### 2. Experimenting Different Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-consensus",
   "metadata": {},
   "source": [
    "#### Approach 1. Pandas Exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "accredited-accused",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 7457.48 MiB, increment: 4716.52 MiB\n",
      "CPU times: user 43.8 s, sys: 7.81 s, total: 51.6 s\n",
      "Wall time: 54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "#simple pandas: read the entire dataset into memory\n",
    "df = pd.read_csv(\"../data/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "transsexual-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Comment out the pandas exchange due to memory limitation.\n",
    "#%%time\n",
    "#%%R -i df\n",
    "### Transferring the python dataframe to R\n",
    "#start_time <- Sys.time()\n",
    "#library(dplyr)\n",
    "#print(class(df))\n",
    "#result <- df %>% count(model)\n",
    "#print(result)\n",
    "#end_time <- Sys.time()\n",
    "#print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-beginning",
   "metadata": {},
   "source": [
    ">  \n",
    "We were not able to run the above code due to memory limitation. We believe that the cause of this issue is due to time and memory spent on serialization and deserialization during file transfers from Pandas to R."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-ivory",
   "metadata": {},
   "source": [
    "#### Approach 2. Arrow Exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "interstate-lecture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 5357.58 MiB, increment: 3950.18 MiB\n",
      "CPU times: user 16.9 s, sys: 14.2 s, total: 31 s\n",
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "dataset = ds.dataset(\"../data/combined_data.csv\", format=\"csv\")\n",
    "## this is of arrow table format\n",
    "table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "technological-image",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5695\n",
      "rarrow.ChunkedArray: 0.027658939361572266\n",
      "5695\n",
      "rarrow.ChunkedArray: 0.021301984786987305\n",
      "5695\n",
      "rarrow.ChunkedArray: 0.027105093002319336\n",
      "5695\n",
      "rarrow.ChunkedArray: 0.02475905418395996\n",
      "5695\n",
      "rarrow.ChunkedArray: 0.029931068420410156\n",
      "5695\n",
      "rarrow.ChunkedArray: 0.021418094635009766\n",
      "5695\n",
      "rarrow.ChunkedArray: 0.02129817008972168\n",
      "peak memory: 4328.71 MiB, increment: 237.41 MiB\n",
      "CPU times: user 18.4 s, sys: 838 ms, total: 19.3 s\n",
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "## Here we are loading the arrow dataframe that we have loaded previously\n",
    "r_table = pyra.converter.py2rpy(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "antique-pioneer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Table\"       \"ArrowObject\" \"R6\"         \n",
      "[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n",
      "\u001b[90m# A tibble: 27 x 2\u001b[39m\n",
      "   model                  n\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m              \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m ACCESS-CM2       1\u001b[4m9\u001b[24m\u001b[4m3\u001b[24m\u001b[4m2\u001b[24m840\n",
      "\u001b[90m 2\u001b[39m ACCESS-ESM1-5    1\u001b[4m6\u001b[24m\u001b[4m1\u001b[24m\u001b[4m0\u001b[24m700\n",
      "\u001b[90m 3\u001b[39m AWI-ESM-1-1-LR    \u001b[4m9\u001b[24m\u001b[4m6\u001b[24m\u001b[4m6\u001b[24m420\n",
      "\u001b[90m 4\u001b[39m BCC-CSM2-MR      3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m5\u001b[24m340\n",
      "\u001b[90m 5\u001b[39m BCC-ESM1          \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m 6\u001b[39m CanESM5           \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m 7\u001b[39m CMCC-CM2-HR4     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 8\u001b[39m CMCC-CM2-SR5     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 9\u001b[39m CMCC-ESM2        3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m10\u001b[39m EC-Earth3-Veg-LR 3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m7\u001b[24m320\n",
      "\u001b[90m# … with 17 more rows\u001b[39m\n",
      "Time difference of 7.949494 secs\n",
      "CPU times: user 9.19 s, sys: 9.8 s, total: 19 s\n",
      "Wall time: 8.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R -i r_table\n",
    "start_time <- Sys.time()\n",
    "print(class(r_table))\n",
    "library(dplyr)\n",
    "result <- r_table %>% collect() %>% count(model)\n",
    "print(class(r_table %>% collect()))\n",
    "end_time <- Sys.time()\n",
    "print(result)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-compression",
   "metadata": {},
   "source": [
    "#### Approach 3. Feather File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "tender-dominant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n",
      "\u001b[90m# A tibble: 27 x 2\u001b[39m\n",
      "   model                  n\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m              \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m ACCESS-CM2       1\u001b[4m9\u001b[24m\u001b[4m3\u001b[24m\u001b[4m2\u001b[24m840\n",
      "\u001b[90m 2\u001b[39m ACCESS-ESM1-5    1\u001b[4m6\u001b[24m\u001b[4m1\u001b[24m\u001b[4m0\u001b[24m700\n",
      "\u001b[90m 3\u001b[39m AWI-ESM-1-1-LR    \u001b[4m9\u001b[24m\u001b[4m6\u001b[24m\u001b[4m6\u001b[24m420\n",
      "\u001b[90m 4\u001b[39m BCC-CSM2-MR      3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m5\u001b[24m340\n",
      "\u001b[90m 5\u001b[39m BCC-ESM1          \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m 6\u001b[39m CanESM5           \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m 7\u001b[39m CMCC-CM2-HR4     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 8\u001b[39m CMCC-CM2-SR5     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 9\u001b[39m CMCC-ESM2        3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m10\u001b[39m EC-Earth3-Veg-LR 3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m7\u001b[24m320\n",
      "\u001b[90m# … with 17 more rows\u001b[39m\n",
      "Time difference of 12.87811 secs\n",
      "CPU times: user 10.6 s, sys: 13.3 s, total: 23.9 s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "library(arrow)\n",
    "start_time <- Sys.time()\n",
    "r_table <- arrow::read_feather(\"../data/combined_data.feather\")\n",
    "print(class(r_table))\n",
    "library(dplyr)\n",
    "result <- r_table %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "print(result)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-providence",
   "metadata": {},
   "source": [
    "#### Approach 4. Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "stone-going",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n",
      "\u001b[90m# A tibble: 27 x 2\u001b[39m\n",
      "   model                  n\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m              \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m ACCESS-CM2       1\u001b[4m9\u001b[24m\u001b[4m3\u001b[24m\u001b[4m2\u001b[24m840\n",
      "\u001b[90m 2\u001b[39m ACCESS-ESM1-5    1\u001b[4m6\u001b[24m\u001b[4m1\u001b[24m\u001b[4m0\u001b[24m700\n",
      "\u001b[90m 3\u001b[39m AWI-ESM-1-1-LR    \u001b[4m9\u001b[24m\u001b[4m6\u001b[24m\u001b[4m6\u001b[24m420\n",
      "\u001b[90m 4\u001b[39m BCC-CSM2-MR      3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m5\u001b[24m340\n",
      "\u001b[90m 5\u001b[39m BCC-ESM1          \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m 6\u001b[39m CanESM5           \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m 7\u001b[39m CMCC-CM2-HR4     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 8\u001b[39m CMCC-CM2-SR5     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 9\u001b[39m CMCC-ESM2        3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m10\u001b[39m EC-Earth3-Veg-LR 3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m7\u001b[24m320\n",
      "\u001b[90m# … with 17 more rows\u001b[39m\n",
      "Time difference of 8.777983 secs\n",
      "CPU times: user 10.2 s, sys: 6.58 s, total: 16.7 s\n",
      "Wall time: 8.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "library(arrow)\n",
    "start_time <- Sys.time()\n",
    "r_table <- arrow::read_parquet(\"../data/combined_data.parquet\")\n",
    "print(class(r_table))\n",
    "library(dplyr)\n",
    "result <- r_table %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "print(result)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-update",
   "metadata": {},
   "source": [
    "### 3. Discussion on Observations\n",
    "- Comparing to 5.6G of csv file, feather file formate takes 1.1G, while parquet file formate only takes 542M. Both feather file and parquet file are more space efficient than csv file.\n",
    "- Exchanging data to R with Pandas, my computer ran out of memory and failed to exchange the data.\n",
    "- Exchanging data to R with Arrow Exchange took 20s. EDA in R using the data table from Arrow took 9s. Total is 29s. \n",
    "- Exchanging data to R with Feather File and performing an EDA took 12s. \n",
    "- Exchanging data to R with Parquet File and performing an EDA took 9s.       \n",
    "\n",
    "In this case, we wound choose Parquet File because it is the fastest approach.      \n",
    "If we want to store the data in hard disk, Parquet File format would be the best choice because it used the least space. Exchanging data to R using Parquet File is also fast. The Parquet is column-oriented data storage format. We are only counting by the column `model`. Parquet only read the column `model` therefore greatly minimized the processing time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-assist",
   "metadata": {},
   "source": [
    "## Challenges and Difficulties Faced "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-omaha",
   "metadata": {},
   "source": [
    "- Peak Memory Seems to be Changing\n",
    "    - The peak memory usage seem to vary on each of team member's machines by a lot. We believe it is because of differnt backgroud applications that are taking up the RAM space. \n",
    "- Combining and Creating the CSV File\n",
    "    - Nash's laptop initially had storage issues due to hard drive being close to full storage. Nash had to free up space before he was successfully able to create combined_data.csv\n",
    "    \n",
    "- Pandas Exchange\n",
    "    - We were not able to exchange the combined data from Pandas to R due to memory limitation. We believe that the cause of this issue is due to time and memory spent on serialization and deserialization during file transfers.\n",
    "    \n",
    "- Arrow Exchange\n",
    "    - For Arrow Exchange, we had to manually add the wall time of converting to R and EDA code together because the converting function is from a Python package and the EDA is done in R. This is different comparing with Feather or Parquet approaches.  We were hoping that we can run the Python code and R code together in one cell so we can measure the memory usage together. However, the kernel restarts every time we tried to run. \n",
    "\n",
    "- Running The Notebook on Team Members' Computer\n",
    "    - The notebook can only be run through successfully on Nick's computer. For Nash, since his computer has only 8GB in RAM, it fails load the combined_data.csv as the peak memory usage is more than 8,000 MiB. The same issue persists on all the other cells where the peak memory requires more than 8,000 MiB. \n",
    "    \n",
    "    - On Huanhuan's computer, EDA in R using feather file failed to run through. The CPU time for her to run takes more than 20 mintues and there are no error message after running.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525]",
   "language": "python",
   "name": "conda-env-525-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
