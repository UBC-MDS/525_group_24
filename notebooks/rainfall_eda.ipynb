{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "disturbed-distance",
   "metadata": {},
   "source": [
    "# DSCI 525 - Web and Cloud Computing\n",
    "## Project: Daily Rainfall Over NSW, Australia\n",
    "## Milestone 1: Tackling Big Data on Your Laptop \n",
    "### Authors: Group 24 Huanhuan Li, Nash Makhija and Nicholas Wu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "designing-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd\n",
    "from memory_profiler import memory_usage\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aerial-flashing",
   "metadata": {},
   "outputs": [],
   "source": [
    "## install the packages https://arrow.apache.org/docs/python/install.html\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "## How to install put instructions https://anaconda.org/conda-forge/rpy2\n",
    "import rpy2.rinterface\n",
    "# install this https://pypi.org/project/rpy2-arrow/#description  pip install rpy2-arrow\n",
    "# have to install this as well conda install -c conda-forge r-arrow \n",
    "import rpy2_arrow.pyarrow_rarrow as pyra\n",
    "### instruction\n",
    "import pyarrow.feather as feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "union-waste",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lihua\\miniconda3\\envs\\525\\lib\\site-packages\\rpy2\\robjects\\packages.py:366: UserWarning: The symbol 'quartz' is not in this R namespace/package.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "least-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for this notebook was adapted from DSCI 525 course notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-affect",
   "metadata": {},
   "source": [
    "## 1) Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "above-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary metadata\n",
    "article_id = 14096681  # this is the unique identifier of the article on figshare\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unnecessary-greece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_link_only': False,\n",
       "  'name': 'daily_rainfall_2014.png',\n",
       "  'supplied_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'computed_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'id': 26579150,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579150',\n",
       "  'size': 58863},\n",
       " {'is_link_only': False,\n",
       "  'name': 'environment.yml',\n",
       "  'supplied_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'computed_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'id': 26579171,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579171',\n",
       "  'size': 192},\n",
       " {'is_link_only': False,\n",
       "  'name': 'README.md',\n",
       "  'supplied_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'computed_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'id': 26586554,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26586554',\n",
       "  'size': 5422},\n",
       " {'is_link_only': False,\n",
       "  'name': 'data.zip',\n",
       "  'supplied_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'computed_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'id': 26766812,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766812',\n",
       "  'size': 814041183},\n",
       " {'is_link_only': False,\n",
       "  'name': 'get_data.py',\n",
       "  'supplied_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'computed_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'id': 26766815,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766815',\n",
       "  'size': 4113}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  # this contains all the articles data, feel free to check it out\n",
    "files = data[\"files\"]             # this is just the data about the files, which is what we want\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "latter-branch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "files_to_dl = [\"data.zip\"]\n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "contemporary-transmission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-luxury",
   "metadata": {},
   "source": [
    "## 2) Combine data CSVs\n",
    "\n",
    "CSVs were combined using `Pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "imported-applicant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>3.293256e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.00</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.50</td>\n",
       "      <td>1.047658e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932835</th>\n",
       "      <td>2014-12-27 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>2.951144e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932836</th>\n",
       "      <td>2014-12-28 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>2.257118e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932837</th>\n",
       "      <td>2014-12-29 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>1.204670e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932838</th>\n",
       "      <td>2014-12-30 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>2.632404e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932839</th>\n",
       "      <td>2014-12-31 12:00:00</td>\n",
       "      <td>-30.00</td>\n",
       "      <td>-28.75</td>\n",
       "      <td>151.875</td>\n",
       "      <td>153.75</td>\n",
       "      <td>3.431610e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1932840 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        time  lat_min  lat_max  lon_min  lon_max  \\\n",
       "0        1889-01-01 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "1        1889-01-02 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "2        1889-01-03 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "3        1889-01-04 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "4        1889-01-05 12:00:00   -36.25   -35.00  140.625   142.50   \n",
       "...                      ...      ...      ...      ...      ...   \n",
       "1932835  2014-12-27 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "1932836  2014-12-28 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "1932837  2014-12-29 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "1932838  2014-12-30 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "1932839  2014-12-31 12:00:00   -30.00   -28.75  151.875   153.75   \n",
       "\n",
       "         rain (mm/day)  \n",
       "0         3.293256e-13  \n",
       "1         0.000000e+00  \n",
       "2         0.000000e+00  \n",
       "3         0.000000e+00  \n",
       "4         1.047658e-02  \n",
       "...                ...  \n",
       "1932835   2.951144e-02  \n",
       "1932836   2.257118e-01  \n",
       "1932837   1.204670e-01  \n",
       "1932838   2.632404e-02  \n",
       "1932839   3.431610e-02  \n",
       "\n",
       "[1932840 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### just listing to get an idea how individual file looks like \n",
    "use_cols = ['time', 'lat_min', 'lat_max', 'lon_min', 'lon_max', 'rain (mm/day)']\n",
    "df = pd.read_csv(\"../data/ACCESS-CM2_daily_rainfall_NSW.csv\", usecols=use_cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-calcium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 425.69 MiB, increment: 0.09 MiB\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "# Shows time that regular python takes to merge file\n",
    "# Join all data together\n",
    "## here we are using a normal python way of merging the data \n",
    "files = glob.glob('../data/*NSW.csv')\n",
    "df = pd.concat((pd.read_csv(file, index_col=0, usecols=use_cols)\n",
    "                .assign(model=file[8:file.index(\"_daily\")])\n",
    "                for file in files)\n",
    "              )\n",
    "df.to_csv(\"../data/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_pandas = pd.read_csv(\"../data/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-kansas",
   "metadata": {},
   "source": [
    "### 2. Summary of Observation on Run Times and Memory Usage Comparison on Different Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-posting",
   "metadata": {},
   "source": [
    "#### Team member comparison:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-subscription",
   "metadata": {},
   "source": [
    "Huanhuan:Total time taken to concatenate and create combined_data.csv was 6min 9s with peak memory usage of 293 MiB. Time taken to read combined_data.csv into pandas was 1min 6s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-resolution",
   "metadata": {},
   "source": [
    "Nash: Total time taken to concatenate and create combined_data.csv was 6min 7s with peak memory usage of 359.25 MiB. Time taken to read combined_data.csv into pandas was 1min 15s. Initially had storage issues due to hard drive being close to full storage, had to free up space before I was successfully able to create combined_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-trainer",
   "metadata": {},
   "source": [
    "Nicholas: Total time taken to concatenate and create combined_data.csv was 4min 44s with peak memory usage of 397 MiB. Time taken to read combined_data.csv into pandas was 50.5s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-envelope",
   "metadata": {},
   "source": [
    "## 3) Load the Combined CSV to Memory and Perform a Simple EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-shape",
   "metadata": {},
   "source": [
    "### Approach 1. Load the Entire Data to Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "#simple pandas - This is how we do normally ,which means we are loading the entire data to the memory\n",
    "df_pandas = pd.read_csv(\"../data/combined_data.csv\")\n",
    "print(df_pandas[\"model\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-questionnaire",
   "metadata": {},
   "source": [
    "\n",
    "### Approach 2. Changing `dtype` of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Memory usage with float64: {df_pandas[['lat_min','lat_max','lon_min', 'lon_max', 'rain (mm/day)']].memory_usage().sum() / 1e6:.2f} MB\")\n",
    "print(f\"Memory usage with float32: {df_pandas[['lat_min','lat_max','lon_min', 'lon_max', 'rain (mm/day)']].astype('float32', errors='ignore').memory_usage().sum() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-safety",
   "metadata": {},
   "source": [
    "### Approach 3. Loading in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "counts = pd.Series(dtype=int)\n",
    "for chunk in pd.read_csv(\"../data/combined_data.csv\", chunksize=10_000_000):\n",
    "    counts = counts.add(chunk[\"model\"].value_counts(), fill_value=0)\n",
    "print(counts.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-consent",
   "metadata": {},
   "source": [
    "### Approach 4. Load using DASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# dask way\n",
    "\n",
    "df_dask = dd.read_csv(\"../data/combined_data.csv\")\n",
    "print(df_dask[\"model\"].value_counts().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-survivor",
   "metadata": {},
   "source": [
    "### 3. Discussion on Observations\n",
    "\n",
    "- Loading the entire data to memory takes the longest in wall time. \n",
    "- If we change those columns with float64 data type to float32, the memory usage reduced significantly from 2,498 MB to 1,249 MB.\n",
    "- Loading in chunks reduced total CPU and sys time as well as wall time. \n",
    "- Loading with DASK reduced wall time significantly by almost half, but we also noticed that the CPU and sys time became greater than wall time, which is likely because DASK loads the data parallely. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-hawaiian",
   "metadata": {},
   "source": [
    "## 4) Perform a Simple EDA in R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-major",
   "metadata": {},
   "source": [
    "### 1. Store the data in different formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "#just seeing if its available\n",
    "library(\"arrow\")\n",
    "library(\"dplyr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-fantasy",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "## read more on the datasets here  https://arrow.apache.org/docs/python/dataset.html\n",
    "dataset = ds.dataset(\"../data/combined_data.csv\", format=\"csv\")\n",
    "## this is of arrow table format\n",
    "table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-system",
   "metadata": {},
   "source": [
    "#### feather format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# experiment in writing in feather format \n",
    "feather.write_feather(table, '../data/example.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-eleven",
   "metadata": {},
   "source": [
    "#### parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-alabama",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## writing as a single parquet \n",
    "pq.write_table(table, '../data/example.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## writing as a partitioned parquet \n",
    "pq.write_to_dataset(table, '../data/example_partitioned.parquet',partition_cols=['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-velvet",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Check the size of different format\n",
    "du -sh ../data/combined_data.csv\n",
    "du -sh ../data/example.feather\n",
    "du -sh ../data/example.parquet\n",
    "du -sh ../data/example_partitioned.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-perception",
   "metadata": {},
   "source": [
    "### 2. Experimenting different approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-aspect",
   "metadata": {},
   "source": [
    "#### Approach 1. Pandas Exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "#simple pandas: read the entire dataset into memory\n",
    "df = pd.read_csv(\"../data/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-hygiene",
   "metadata": {},
   "outputs": [],
   "source": [
    "##I comment out the pandas exchange due to memory limitation.\n",
    "#%%time\n",
    "#%%R -i df\n",
    "### Transferring the python dataframe to R\n",
    "#start_time <- Sys.time()\n",
    "#library(dplyr)\n",
    "#print(class(df))\n",
    "#result <- df %>% count(model)\n",
    "#print(result)\n",
    "#end_time <- Sys.time()\n",
    "#print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-enforcement",
   "metadata": {},
   "source": [
    "#### Approach 2. Arrow Exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "dataset = ds.dataset(\"../data/combined_data.csv\", format=\"csv\")\n",
    "## this is of arrow table format\n",
    "table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "## Here we are loading the arrow dataframe that we have loaded previously\n",
    "r_table = pyra.converter.py2rpy(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R -i r_table\n",
    "start_time <- Sys.time()\n",
    "print(class(r_table))\n",
    "##add details on collect here\n",
    "library(dplyr)\n",
    "# Arrow only support some operations check this out https://arrow.apache.org/docs/r/articles/dataset.html\n",
    "result <- r_table %>% collect() %>% count(model)\n",
    "print(class(r_table %>% collect()))\n",
    "end_time <- Sys.time()\n",
    "print(result)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-ordering",
   "metadata": {},
   "source": [
    "#### Approach 3. Feather File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "### her we are showing how much time it took to read a feather file what we wrote in python\n",
    "library(arrow)\n",
    "start_time <- Sys.time()\n",
    "r_table <- arrow::read_feather(\"../data/example.feather\")\n",
    "print(class(r_table))\n",
    "library(dplyr)\n",
    "result <- r_table %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "print(result)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-consideration",
   "metadata": {},
   "source": [
    "#### Approach 4. Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "### her we are showing how much time it took to read a parquet file what we wrote in python\n",
    "library(arrow)\n",
    "start_time <- Sys.time()\n",
    "r_table <- arrow::read_parquet(\"../data/example.parquet\")\n",
    "print(class(r_table))\n",
    "library(dplyr)\n",
    "result <- r_table %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "print(result)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "### her we are showing how much time it took to read a parquet partitioned file what we wrote in python\n",
    "library(arrow)\n",
    "start_time <- Sys.time()\n",
    "r_table <- arrow::read_parquet(\"../data/example_partitioned.parquet\")\n",
    "print(class(r_table))\n",
    "library(dplyr)\n",
    "result <- r_table %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "print(result)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-enough",
   "metadata": {},
   "source": [
    "### 3. Discussion on Observations\n",
    "- Comparing to 5.7G of csv file, feather file formate takes 1.1G, while parquet file formate only takes 542M. Both feather file and parquet file are more space efficent than csv file.\n",
    "- Exchanging data to R with Pandas, my computer ran out of memory and failed to exchange the data.\n",
    "- Exchanging data to R with Arrow Exchange, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-upgrade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
